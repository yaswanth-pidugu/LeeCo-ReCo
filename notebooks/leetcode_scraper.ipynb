{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LeetCode Problem Scraper & Dataset Builder\n",
    "\n",
    "This project builds a **complete dataset of LeetCode problems** directly from the official GraphQL API.\n",
    "\n",
    "### What It Does\n",
    "- Connects to LeetCode’s GraphQL endpoint using authenticated sessions\n",
    "- Fetches problem metadata (title, difficulty, tags, stats, etc.)\n",
    "- Extracts **descriptions, similar problems, and solution URLs**\n",
    "- Saves everything into a structured CSV file for analysis or dashboards\n",
    "\n",
    "### Goals\n",
    "- Maintain an up-to-date, queryable dataset for personal analytics and research\n",
    "- Support future automation for weekly updates\n",
    "- Enable quick data exploration and ML-based recommendation experiments\n",
    "\n",
    "> This notebook is designed as a **step-by-step guide** — part scraper, part documentation."
   ],
   "id": "a16dd30a1bd87cb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T16:51:00.418636Z",
     "start_time": "2025-10-26T16:51:00.410131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests, json, time, random, pandas as pd, os\n",
    "from requests.adapters import HTTPAdapter\n",
    "from datetime import datetime\n",
    "try:\n",
    "    from urllib3.util.retry import Retry\n",
    "except Exception:\n",
    "    from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "GRAPHQL_URL = \"https://leetcode.com/graphql/\"\n",
    "HOMEPAGE = \"https://leetcode.com/problemset/\""
   ],
   "id": "a3fc26aa2b64d1f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Session Setup\n",
    "\n",
    "We create a persistent `requests.Session()` with:\n",
    "- Retry mechanism (to handle rate limits and 5xx errors)\n",
    "- Proper CSRF token handling\n",
    "- Headers that mimic a browser\n",
    "\n",
    "This ensures stable and polite communication with LeetCode’s servers.\n"
   ],
   "id": "413f87ffbb1209dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T16:51:07.973035Z",
     "start_time": "2025-10-26T16:51:07.958484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_leetcode_session():\n",
    "    s = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=5, connect=5, read=5,\n",
    "        backoff_factor=1.2,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=frozenset([\"GET\", \"POST\"]),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry, pool_connections=20, pool_maxsize=20)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.mount(\"http://\", adapter)\n",
    "\n",
    "    r = s.get(HOMEPAGE, headers={\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"Referer\": \"https://leetcode.com/\",\n",
    "        \"Origin\": \"https://leetcode.com\",\n",
    "        \"Accept\": \"application/json, text/plain, */*\",\n",
    "    }, timeout=(10, 30))\n",
    "    csrftoken = s.cookies.get(\"csrftoken\", \"\")\n",
    "    s.headers.update({\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"Referer\": \"https://leetcode.com/\",\n",
    "        \"Origin\": \"https://leetcode.com\",\n",
    "        \"Accept\": \"application/json, text/plain, */*\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"x-csrftoken\": csrftoken,\n",
    "    })\n",
    "    return s"
   ],
   "id": "f0f867d6fdb29423",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## GraphQL Query Handler\n",
    "\n",
    "LeetCode exposes a single `/graphql/` endpoint.\n",
    "All requests — from problem listing to details — go through it.\n",
    "\n",
    "We define a helper function that:\n",
    "- Accepts a query + variables\n",
    "- Retries failed requests\n",
    "- Returns JSON data directly\n",
    "\n",
    "This makes all downstream queries clean and reusable.\n"
   ],
   "id": "459863118b550e38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T16:51:14.718404Z",
     "start_time": "2025-10-26T16:51:14.699993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def graphql_query(session, query, variables=None, max_retries=4):\n",
    "    payload = {\"query\": query}\n",
    "    if variables:\n",
    "        payload[\"variables\"] = variables\n",
    "\n",
    "    last_err = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            r = session.post(GRAPHQL_URL, json=payload, timeout=(10, 60))\n",
    "            js = r.json()\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep((2 ** (attempt - 1)) + random.uniform(0, 0.6))\n",
    "            continue\n",
    "\n",
    "        if \"errors\" in js:\n",
    "            last_err = RuntimeError(js[\"errors\"][0].get(\"message\", \"GraphQL error\"))\n",
    "            time.sleep((2 ** (attempt - 1)) + random.uniform(0, 0.6))\n",
    "            continue\n",
    "\n",
    "        if \"data\" in js:\n",
    "            return js[\"data\"]\n",
    "\n",
    "        last_err = RuntimeError(f\"Unexpected response: {r.status_code} {r.text[:300]}\")\n",
    "        time.sleep((2 ** (attempt - 1)) + random.uniform(0, 0.6))\n",
    "    raise last_err or RuntimeError(\"GraphQL request failed\")"
   ],
   "id": "9b7437fdf8a86478",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## GraphQL Queries\n",
    "\n",
    "We’ll use two main queries:\n",
    "\n",
    "1. **PROBLEMSET_QUERY** – Lists problems in paginated chunks\n",
    "2. **QUESTION_DETAIL_QUERY** – Fetches full details for a single problem\n",
    "\n",
    "These are defined once and reused throughout the scraper.\n"
   ],
   "id": "82203f39e19bc99b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-26T16:51:21.292592Z",
     "start_time": "2025-10-26T16:51:21.283755Z"
    }
   },
   "source": [
    "PROBLEMSET_QUERY = \"\"\"\n",
    "query problemsetQuestionList($categorySlug: String, $limit: Int, $skip: Int, $filters: QuestionListFilterInput) {\n",
    "  problemsetQuestionList: questionList(categorySlug: $categorySlug, limit: $limit, skip: $skip, filters: $filters) {\n",
    "    total: totalNum\n",
    "    questions: data {\n",
    "      questionFrontendId\n",
    "      title\n",
    "      titleSlug\n",
    "      difficulty\n",
    "      acRate\n",
    "      isPaidOnly\n",
    "      topicTags { name slug }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "QUESTION_DETAIL_QUERY = \"\"\"\n",
    "query questionData($titleSlug: String!) {\n",
    "  question(titleSlug: $titleSlug) {\n",
    "    questionId\n",
    "    questionFrontendId\n",
    "    title\n",
    "    titleSlug\n",
    "    difficulty\n",
    "    isPaidOnly\n",
    "    acRate\n",
    "    content\n",
    "    stats\n",
    "    likes\n",
    "    dislikes\n",
    "    topicTags { name slug }\n",
    "    similarQuestions\n",
    "    discussionCount\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fetching Problem Data\n",
    "\n",
    "Here’s where the real scraping happens.\n",
    "\n",
    "- The scraper loops through all problem pages\n",
    "- For each problem, it fetches detailed stats (likes, tags, discussions, etc.)\n",
    "- The data is collected and periodically saved to a checkpoint CSV\n",
    "\n",
    "We also ensure duplicate prevention by checking existing entries.\n"
   ],
   "id": "edf0ad42dfee8f6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T16:51:30.621607Z",
     "start_time": "2025-10-26T16:51:30.588060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_all_problems_df(page_size=50, checkpoint_path=None):\n",
    "    session = make_leetcode_session()\n",
    "    all_rows = []\n",
    "    skip = 0\n",
    "    total = None\n",
    "\n",
    "    seen = set()\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        old_df = pd.read_csv(checkpoint_path)\n",
    "        seen = set(old_df['titleSlug'])\n",
    "        all_rows = old_df.to_dict('records')\n",
    "        print(f\"Loaded {len(seen)} existing problems from {checkpoint_path}\")\n",
    "\n",
    "    while True:\n",
    "        variables = {\"categorySlug\": \"\", \"limit\": page_size, \"skip\": skip, \"filters\": {}}\n",
    "        data = graphql_query(session, PROBLEMSET_QUERY, variables)\n",
    "        root = data[\"problemsetQuestionList\"]\n",
    "        if total is None:\n",
    "            total = root[\"total\"] or 0\n",
    "        batch = root[\"questions\"] or []\n",
    "        if not batch:\n",
    "            break\n",
    "\n",
    "        for q in batch:\n",
    "            slug = q[\"titleSlug\"]\n",
    "            if slug in seen:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                detail_data = graphql_query(session, QUESTION_DETAIL_QUERY, {\"titleSlug\": slug})\n",
    "                qd = detail_data[\"question\"]\n",
    "\n",
    "                stats = json.loads(qd.get(\"stats\", \"{}\"))\n",
    "                similar = json.loads(qd.get(\"similarQuestions\", \"[]\"))\n",
    "\n",
    "                row = {\n",
    "                    \"frontend_id\": qd.get(\"questionFrontendId\"),\n",
    "                    \"internal_id\": qd.get(\"questionId\"),\n",
    "                    \"title\": qd.get(\"title\"),\n",
    "                    \"titleSlug\": slug,\n",
    "                    \"difficulty\": qd.get(\"difficulty\"),\n",
    "                    \"is_premium\": qd.get(\"isPaidOnly\"),\n",
    "                    \"topic_tags\": [t[\"name\"] for t in qd.get(\"topicTags\", [])],\n",
    "                    \"similar_questions\": [s.get(\"title\") for s in similar] if similar else [],\n",
    "                    \"no_similar_questions\": len(similar),\n",
    "                    \"acceptance\": qd.get(\"acRate\"),\n",
    "                    \"accepted\": stats.get(\"totalAcceptedRaw\"),\n",
    "                    \"submission\": stats.get(\"totalSubmissionRaw\"),\n",
    "                    \"discussion_count\": qd.get(\"discussionCount\"),\n",
    "                    \"likes\": qd.get(\"likes\"),\n",
    "                    \"dislikes\": qd.get(\"dislikes\"),\n",
    "                    \"description\": qd.get(\"content\", \"\"),\n",
    "                    \"problem_URL\": f\"https://leetcode.com/problems/{slug}/\",\n",
    "                    \"solution_URL\": f\"https://leetcode.com/problems/{slug}/solution/\" if not qd.get(\"isPaidOnly\") else None,\n",
    "                    \"last_updated\": datetime.now().strftime(\"%Y-%m-%d\")\n",
    "                }\n",
    "\n",
    "                all_rows.append(row)\n",
    "                seen.add(slug)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {slug}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if checkpoint_path:\n",
    "            pd.DataFrame(all_rows).to_csv(checkpoint_path, index=False)\n",
    "\n",
    "        skip += page_size\n",
    "        if len(seen) >= total:\n",
    "            break\n",
    "        time.sleep(random.uniform(0.8, 1.5))  # avoid rate-limit\n",
    "\n",
    "    return pd.DataFrame(all_rows)"
   ],
   "id": "e51615a3ee0ecafe",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Save & Inspect\n",
    "\n",
    "Once scraping is complete, the dataset is stored as `leetcode_latest.csv`.\n",
    "We preview the first few rows to verify integrity and schema.\n",
    "\n",
    "> Tip: Each record includes both internal and frontend problem IDs.\n"
   ],
   "id": "b65e1dc66fe9a025"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T16:51:40.762121Z",
     "start_time": "2025-10-26T16:51:40.756983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def scrape(file_name=\"leetcode_full.csv\"):\n",
    "    df = fetch_all_problems_df(page_size=50, checkpoint_path=file_name)\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"Scraping complete! {len(df)} problems saved to {file_name}\")"
   ],
   "id": "6d6f918ff7e3410a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T17:18:38.227893Z",
     "start_time": "2025-10-26T16:52:03.002499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    scrape(\"../data/raw/leetcode_latest.csv\")"
   ],
   "id": "8c59fe39acbdc01a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete! 3730 problems saved to leetcode_latest.csv\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Future Plans\n",
    "\n",
    "- Integrate PostgreSQL for scalable querying\n",
    "- Add automatic weekly refresh and delta-tracking\n",
    "- Visualize trends via dashboards (Reflex, Streamlit, or Plotly)\n",
    "- Extend scraper to fetch company tags and problem frequency stats\n"
   ],
   "id": "c64b5409d6b395b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## References & Resources\n",
    "\n",
    "Below are key references used in developing this LeetCode scraper and dataset builder:\n",
    "\n",
    "### Official Documentation\n",
    "- [LeetCode GraphQL Endpoint Overview (Unofficial Discussion)](https://leetcode.com/graphql)\n",
    "- [LeetCode Problemset Page (Used to Initialize Session)](https://leetcode.com/problemset/)\n",
    "- [LeetCode API Reference – Community Wiki](https://github.com/skygragon/leetcode-cli/wiki/API)\n",
    "- [LeetCode Explore API Explanation](https://github.com/chenjianhui96/leetcode-graphql-docs) *(community maintained)*\n",
    "\n",
    "### Python Libraries\n",
    "- [Requests — Python HTTP for Humans](https://docs.python-requests.org/en/master/)\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [urllib3 Retry Mechanism](https://urllib3.readthedocs.io/en/stable/reference/urllib3.util.html#urllib3.util.retry.Retry)\n",
    "- [HTTPAdapter for Session Handling](https://requests.readthedocs.io/en/latest/api/#requests.adapters.HTTPAdapter)\n",
    "\n",
    "### Related Articles / Blogs\n",
    "- [How to Use LeetCode’s GraphQL API — by Taranjeet Singh](https://medium.com/@taranjeet/how-to-use-leetcodes-graphql-api-bd5f3f0a6a61)\n",
    "- [Building a Web Scraper with Python Requests and BeautifulSoup](https://realpython.com/beautiful-soup-web-scraper-python/)\n",
    "- [GraphQL Query Basics](https://graphql.org/learn/queries/)\n",
    "- [Creating a Robust Data Scraper with Retries and Backoff](https://docs.python.org/3/library/retry.html) *(official concepts)*\n",
    "\n",
    "### Dataset Examples (Inspiration)\n",
    "- [Kaggle: LeetCode Questions Dataset (Community Maintained)](https://www.kaggle.com/datasets/kanchana1990/leetcode-questions-dataset)\n",
    "- [LeetCode Problems – Open Source JSON Archive (GitHub)](https://github.com/skygragon/leetcode-cli/blob/master/lib/plugins/leetcode/api.js)\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:**\n",
    "> Always cross-check third-party datasets and documentation — LeetCode doesn’t officially expose a public API, so the GraphQL endpoint can change at any time.\n"
   ],
   "id": "295bff3c876f388"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "65472ffaa68745c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
