{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T04:19:46.489421Z",
     "start_time": "2025-11-07T04:14:06.025277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, re, ast, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "DATA_PATH = Path(r\"C:\\Users\\yashw\\PycharmProjects\\PythonProject4\\data\\processed\\preprocessed_data.csv\")\n",
    "EMB_PATH  = Path(r\"C:\\Users\\yashw\\PycharmProjects\\PythonProject4\\models\\sbert_recommender.pkl\")\n",
    "MODEL_SAVE = Path(r\"C:\\Users\\yashw\\PycharmProjects\\PythonProject4\\models\\lambdarank_model.pkl\")\n",
    "\n",
    "def clean_title(t):\n",
    "    t = str(t).lower().strip()\n",
    "    t = re.sub(r'^\\d+\\.\\s*', '', t)\n",
    "    t = re.sub(r'[^a-z0-9\\s\\-]', '', t)\n",
    "    t = re.sub(r'\\s+', ' ', t)\n",
    "    return t\n",
    "\n",
    "def parse_similar(x):\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            return [clean_title(i) for i in ast.literal_eval(x)]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "def to_tag_list(s):\n",
    "    if isinstance(s, str) and s.startswith('['):\n",
    "        try:\n",
    "            vals = ast.literal_eval(s)\n",
    "            return [v.lower().strip() for v in vals]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return [t.strip().lower() for t in str(s).split(',') if t.strip()]\n",
    "\n",
    "def minmax(x):\n",
    "    x = x.fillna(0)\n",
    "    rng = x.max() - x.min()\n",
    "    return (x - x.min()) / rng if rng != 0 else x * 0\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df['clean_title'] = df['title'].apply(clean_title)\n",
    "df['ground_truth'] = df['similar_questions'].apply(parse_similar)\n",
    "df['tag_list'] = df['topic_tags'].apply(to_tag_list)\n",
    "df['difficulty'] = df['difficulty'].fillna('Medium')\n",
    "\n",
    "if EMB_PATH.exists():\n",
    "    with open(EMB_PATH, \"rb\") as f:\n",
    "        cache = pickle.load(f)\n",
    "    embeddings = np.array(cache[\"embeddings\"], dtype=np.float32)\n",
    "else:\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = model.encode(df['title'].tolist(), show_progress_bar=True, normalize_embeddings=True)\n",
    "    with open(EMB_PATH, \"wb\") as f:\n",
    "        pickle.dump({\"model_name\": \"all-MiniLM-L6-v2\", \"embeddings\": embeddings}, f)\n",
    "\n",
    "N = len(df)\n",
    "\n",
    "ladder = {\"easy\": 0, \"medium\": 1, \"hard\": 2}\n",
    "diff_vals = df['difficulty'].str.lower().map(ladder).fillna(1).to_numpy(dtype=np.int8)\n",
    "\n",
    "acc = minmax(df['acceptance']) if 'acceptance' in df else 0\n",
    "likes = minmax(df['likes']) if 'likes' in df else 0\n",
    "subs = minmax(df['submission']) if 'submission' in df else 0\n",
    "popularity = (0.3 * acc + 0.5 * likes + 0.2 * subs).fillna(0).to_numpy(dtype=np.float32)\n",
    "\n",
    "print(\"Building tag similarity matrix...\")\n",
    "tag_sims = np.zeros((N, N), dtype=np.float32)\n",
    "for i in tqdm(range(N)):\n",
    "    tags_i = set(df.iloc[i]['tag_list'])\n",
    "    for j in range(i + 1, N):\n",
    "        tags_j = set(df.iloc[j]['tag_list'])\n",
    "        if not tags_i or not tags_j:\n",
    "            continue\n",
    "        inter = len(tags_i & tags_j)\n",
    "        uni = len(tags_i | tags_j)\n",
    "        if uni:\n",
    "            val = inter / uni\n",
    "            tag_sims[i, j] = tag_sims[j, i] = val\n",
    "\n",
    "print(\"Building difficulty similarity matrix...\")\n",
    "diff_sims = np.ones((N, N), dtype=np.float32)\n",
    "for i in range(N):\n",
    "    for j in range(i + 1, N):\n",
    "        d = abs(diff_vals[i] - diff_vals[j])\n",
    "        diff_sims[i, j] = diff_sims[j, i] = 1.0 if d == 0 else 0.7 if d == 1 else 0.4\n",
    "\n",
    "\n",
    "query_ids = np.arange(N)\n",
    "query_ids = shuffle(query_ids, random_state=42)\n",
    "train_queries, val_queries = train_test_split(query_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Generating grouped feature data...\")\n",
    "features, labels, group_sizes = [], [], []\n",
    "\n",
    "for i in tqdm(train_queries, desc=\"Building training pairs\"):\n",
    "    gt = df.loc[i, 'ground_truth']\n",
    "    if not gt:\n",
    "        continue\n",
    "    pos_idx = []\n",
    "    for title in gt:\n",
    "        j_match = df.index[df['clean_title'] == title].tolist()\n",
    "        if j_match:\n",
    "            pos_idx.append(j_match[0])\n",
    "    if not pos_idx:\n",
    "        continue\n",
    "\n",
    "    # Positive pairs\n",
    "    for j in pos_idx:\n",
    "        emb_sim = np.dot(embeddings[i], embeddings[j])\n",
    "        tag_sim = tag_sims[i, j]\n",
    "        diff_sim = diff_sims[i, j]\n",
    "        pop_diff = abs(popularity[i] - popularity[j])\n",
    "        features.append([emb_sim, tag_sim, diff_sim, pop_diff])\n",
    "        labels.append(1)\n",
    "\n",
    "    # Negative pairs\n",
    "    rand_idx = np.random.choice(N, size=min(5 * len(pos_idx), N), replace=False)\n",
    "    rand_idx = rand_idx[rand_idx != i]\n",
    "    for j in rand_idx:\n",
    "        emb_sim = np.dot(embeddings[i], embeddings[j])\n",
    "        tag_sim = tag_sims[i, j]\n",
    "        diff_sim = diff_sims[i, j]\n",
    "        pop_diff = abs(popularity[i] - popularity[j])\n",
    "        features.append([emb_sim, tag_sim, diff_sim, pop_diff])\n",
    "        labels.append(0)\n",
    "\n",
    "    group_sizes.append(len(pos_idx) + len(rand_idx))\n",
    "\n",
    "X = np.array(features, dtype=np.float32)\n",
    "y = np.array(labels, dtype=np.float32)\n",
    "print(f\"Training samples: {len(X)}, Groups: {len(group_sizes)}\")\n",
    "\n",
    "train_data = lgb.Dataset(X, label=y, group=group_sizes)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\": \"ndcg\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 63,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"ndcg_eval_at\": [10],\n",
    "    \"verbosity\": -1\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=[train_data],\n",
    "    valid_names=['train'],\n",
    "    callbacks=[\n",
    "        lgb.log_evaluation(50)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nFeature importances:\")\n",
    "for n, imp in zip([\"emb_sim\", \"tag_sim\", \"diff_sim\", \"pop_diff\"], model.feature_importance()):\n",
    "    print(f\"{n:10s} -> {imp}\")\n",
    "\n",
    "with open(MODEL_SAVE, \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "print(\"LambdaRank model trained and saved successfully.\")\n"
   ],
   "id": "1923010239eb67da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tag similarity matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3735/3735 [05:30<00:00, 11.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building difficulty similarity matrix...\n",
      "Generating grouped feature data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building training pairs: 100%|██████████| 2988/2988 [00:01<00:00, 1860.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 24195, Groups: 1757\n",
      "[50]\ttrain's ndcg@10: 0.962555\n",
      "[100]\ttrain's ndcg@10: 0.973675\n",
      "[150]\ttrain's ndcg@10: 0.98291\n",
      "[200]\ttrain's ndcg@10: 0.989443\n",
      "[250]\ttrain's ndcg@10: 0.992758\n",
      "[300]\ttrain's ndcg@10: 0.995212\n",
      "[350]\ttrain's ndcg@10: 0.996645\n",
      "[400]\ttrain's ndcg@10: 0.997513\n",
      "[450]\ttrain's ndcg@10: 0.998284\n",
      "[500]\ttrain's ndcg@10: 0.998787\n",
      "\n",
      "Feature importances:\n",
      "emb_sim    -> 12136\n",
      "tag_sim    -> 5354\n",
      "diff_sim   -> 1508\n",
      "pop_diff   -> 12002\n",
      "LambdaRank model trained and saved successfully.\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
